---
title: –ö–∏—Ä–∏–ª–ª –ë–æ–±—ã—Ä–µ–≤
cover: https://merkulov.top/Teaching/Optimization_methods/Optimization_methods_____/–õ—É—á—à–∏–µ_–ø—Ä–æ–µ–∫—Ç—ã_–ø–æ_–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏_2018/–ö–∏—Ä–∏–ª–ª_–ë–æ–±—ã—Ä–µ–≤/bobyrev.jpeg
icon: None
emoji: ‚≠ê
Project: Empirical Study of TD $ \gamma $ Reinforcement Learning Algorithm
Code: [üï∏](https://colab.research.google.com/drive/1fmRALZn0-BRKlHWK_juP9sswUdpEtCZ0?authuser=2#scrollTo=6DEOJnl0idB-)
Poster: [üìé](https://merkulov.top/Teaching/Optimization_methods/Optimization_methods_____/–õ—É—á—à–∏–µ_–ø—Ä–æ–µ–∫—Ç—ã_–ø–æ_–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏_2018/–ö–∏—Ä–∏–ª–ª_–ë–æ–±—ã—Ä–µ–≤/bobyrev_poster.pdf)
---

–ö–∏—Ä–∏–ª–ª ~~—É–∂–µ —Å –¥–µ—Ç—Å—Ç–≤–∞~~ —Å –º–ª–∞–¥—à–∏—Ö –∫—É—Ä—Å–æ–≤ —É–≤–ª–µ–∫–∞–µ—Ç—Å—è Reinforcement Learning, —á–∏—Ç–∞–µ—Ç –∫–Ω–∏–∂–∫–∏, —Ö–æ–¥–∏—Ç –Ω–∞ –¥–æ–ø. —Å–µ–º–∏–Ω–∞—Ä—ã –∏ –ø–∏—à–µ—Ç –∫–æ–¥. –í –ø—Ä–æ–µ–∫—Ç–µ –æ–Ω —Å—Ä–∞–≤–Ω–∏–≤–∞–ª –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Temporal Difference learning –∞–ª–≥–æ—Ä–∏—Ç–º —Å –µ–≥–æ –Ω–æ–≤–æ–π –≤–∞—Ä–∏–∞—Ü–∏–µ–π. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –≤ –ø–æ—Å—Ç–µ—Ä–µ –æ–Ω –∫—Ä–∞—Ç–∫–æ –≤–≤–µ–ª –≤ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫—É –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫–ø—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á—Ç–æ —Ç–æ–∂–µ –±—É–¥–µ—Ç –¥–ª—è –º–Ω–æ–≥–∏—Ö –µ–≥–æ –æ–¥–Ω–æ–∫—É—Ä—Å–Ω–∏–∫–æ–≤ –≤ –Ω–æ–≤–∏–Ω–∫—É.
